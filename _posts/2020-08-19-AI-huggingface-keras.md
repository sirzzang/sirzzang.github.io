---
title:  "[AI] Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©í•˜ê¸°"
excerpt: "ì´ì œ HuggingFace Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ Keras Functional APIì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤!"
toc: true
toc_sticky: true
categories:
  - AI
tags:
  - ìì—°ì–´ì²˜ë¦¬
  - NLP
  - Transformers
  - Keras

last_modified_at: 2020-08-19
---

<sup>ì¶œì²˜ : [huggingface.co/transformers](https://huggingface.co/transformers)</sup>

<br>

# _ë” í¸í•´ì§„ HuggingFace Transformers_



<br>

 HuggingFaceì˜ Transformers íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ì˜ ëª¨ë¸ë“¤ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•´ ë†“ì•˜ë‹¤. ì˜ˆì „ì— í”„ë¡œì íŠ¸í•  ë•Œë„ ìœ ìš©í•˜ê²Œ ì¼ì—ˆëŠ”ë°, ê·¸ ë•Œì—ëŠ” PyTorchë¡œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆì—ˆë‹¤. ê·¸ëŸ°ë° ì´ì œëŠ” **Tensorflow 2.x ë²„ì „ì—ì„œ, íŠ¹íˆë‚˜ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ Keras ëª¨ë¸ì²˜ëŸ¼** ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤! 



> *ì°¸ê³ * 
>
> * **Models** are standard [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) or [tf.**keras**.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) so you can use them in your usual training loop. ğŸ¤— 
> * **Model classes** such as [`BertModel`](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel), which are 30+ PyTorch models ([torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) or Keras models ([tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)) that work with the pretrained weights provided in the library.



ì‚¬ì „í•™ìŠµí•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ ì™€ì„œ, Kerasì˜ *functional API*ì²˜ëŸ¼ ì»´íŒŒì¼í•˜ê³ , í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆë‹¤.

 [ì´ ê¸€](https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a)ì„ ì°¸ê³ í•˜ì—¬, ì–´ë–»ê²Œ HuggingFaceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ Keras ëª¨ë¸ì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€ ì •ë¦¬í•´ ë³¸ë‹¤.

 <br>



## 1. ê°œìš”



 ê¸°ë³¸ì ìœ¼ë¡œ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ ì ìš©ë˜ëŠ” í° ì‘ì—… íë¦„ì€ ëª¨ë‘ ë™ì¼í•˜ë‹¤. ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ë¡œë¶€í„° í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ ì™€ ë¬¸ì„œë¥¼ í† í¬ë‚˜ì´ì§•í•˜ê³ , ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ ë’¤ ê·¸ ì¶œë ¥ì¸ ì„ë² ë”©ì„ í™œìš©í•´ íŒŒì¸íŠœë‹í•˜ë©´ ëœë‹¤.

<br>

### Tokenizer

 í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ ì˜¤ê³ , `encode` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ë°”ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì˜ `word2idx`ì— ë”°ë¼ ì¸ì½”ë”©ëœ ìˆ˜ì¹˜ ë²¡í„°ê°€ ë‚˜ì˜¨ë‹¤. BERT ëª¨ë¸ì—ì„œ í•œêµ­ì–´ëŠ” `bert-base-multilingual-cased` ëª¨ë¸ì— ì‚¬ì „ í•™ìŠµë˜ì–´ ìˆìœ¼ë¯€ë¡œ, í•´ë‹¹ ëª¨ë¸ì˜ ì´ë¦„ì„ ì¸ìë¡œ ë„˜ê¸´ë‹¤. ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì˜ ì–´íœ˜ì§‘ì— ë¬´ì—‡ì´ ìˆëŠ”ì§€ ì•Œê³  ì‹¶ë‹¤ë©´, `get_vocab()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•œë‹¤. 

```python
# í† í¬ë‚˜ì´ì € ì„¤ì •
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

# ì–´íœ˜ì§‘ ìƒì„±
word2idx = tokenizer.get_vocab()
idx2word = {idx:word for idx, word in enumerate(word2idx)}

# ë°ì´í„° ì˜ˆì‹œ í™•ì¸
for idx in tokenizer.encode('ë­ì•¼ ì´ í‰ì ë“¤ì€ ë‚˜ì˜ì§„ ì•Šì§€ë§Œ ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„'):
    print(idx2word[idx], end=' ')
```



 ì „ì²˜ë¦¬í•œ ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì €ë¡œ ì¸ì½”ë”©í•˜ê³ , ì–´íœ˜ì§‘ì—ì„œ ì¸ë±ìŠ¤ë¥¼ ì°¾ì•„ ê²°ê³¼ë¥¼ í™•ì¸í•œë‹¤. ë‹¤ìŒê³¼ ê°™ì´ special tokenê³¼ wordpiece í† í¬ë‚˜ì´ì§•ëœ ëª¨ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python
[CLS] ë­ ##ì•¼ ì´ í‰ ##ì  ##ë“¤ì€ ë‚˜ ##ì˜ ##ì§„ ì•Š ##ì§€ë§Œ ì  ì§œ ##ë¦¬ëŠ” ë” ##ë” ##ìš± [UNK] [SEP] 
```

<br>

 í† í¬ë‚˜ì´ì €ì— íŒŒë¼ë¯¸í„°ë¡œ `padding`, `truncating` ë“±ì˜ ì˜µì…˜ì„ ì¤„ ìˆ˜ ìˆë‹¤. ë³„ë„ì˜ ì „ì²˜ë¦¬ë¥¼ ê±°ì¹˜ì§€ ì•Šì•„ë„ í•œ ë²ˆì— ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ë§ì¶œ ìˆ˜ ìˆì–´ í¸ë¦¬í•˜ë‹¤. `return_tensors` ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ Pytorch í˜¹ì€ Tensorflow í˜•íƒœì˜ í…ì„œë¡œ ì¸ì½”ë”©í•œë‹¤. ê²°ê³¼ë¡œëŠ” dictionaryê°€ ë°˜í™˜ëœë‹¤. ì£¼ë¡œ ì‚¬ìš©í•  ê²ƒì€ `input_ids`, `token_type_ids`, `attention_mask` ë“±ì´ë¯€ë¡œ, í•´ë‹¹ í‚¤ë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤.

```python
# ì¸ì½”ë”©
train_encoded = tokenizer(train_sentences, padding=True, truncation=True, max_length=MAX_SEQUENCE_LEN, return_tensors='tf')

# ì¸ì½”ë”©ëœ ë¬¸ì¥
X_train = train_encoded['input_ids']

# attention mask
X_train_masks = train_encoded['attention_mask']

# ë°ì´í„° ì˜ˆì‹œ
print(X_train[0]) # ì¸ì½”ë”©ëœ ë¬¸ì¥
print(tokenizer.decode(X_train[0])) # ë””ì½”ë”©
```

 ì¸ì½”ë”©ëœ ê²°ê³¼ í…ì„œì™€ ê·¸ê²ƒì„ ë””ì½”ë”©í•œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
tf.Tensor(
[   101   9519   9074 119005   9708 119235   9715 119230  16439  77884
  48549   9284  22333  12692    102      0      0      0      0      0
      0      0      0      0      0      0      0      0      0      0
      0      0      0      0      0      0      0      0      0      0
      0      0      0      0      0      0      0      0      0      0
      0      0      0      0      0      0      0      0      0      0
      0      0      0      0], shape=(64,), dtype=int32)
[CLS] ì•„ ë”ë¹™ ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
```



<br>

### Model



 HuggingFaceê°€ ì´ë¯¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì— **ê° ëª©ì ì— ë§ëŠ”, ì–¸ì–´ ëª¨ë¸**ì„ êµ¬í˜„í•´ ë†“ì•˜ë‹¤. ë¶„ë¥˜ ëª¨ë¸ì„ ì˜ˆë¡œ ë“¤ë©´, `BertForSequenceClassification`(*BERT*), `AlbertForSequenceClassification`(*ALBERT*) ì™€ ê°™ì€ ì‹ì´ë‹¤. ê° ì–¸ì–´ ëª¨ë¸ ë° ëª©ì ì„ ì„ íƒí•˜ëŠ” ê²ƒì€ documentationì„ ì°¸ê³ í•˜ë©´ ëœë‹¤.  Pytorchì™€ Tensorflowì—ì„œ ëª¨ë‘ í™œìš©í•  ìˆ˜ ìˆëŠ”ë°, Tensorflow ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì˜ ì´ë¦„ì—ëŠ” `TF`ê°€ ë¶™ëŠ”ë‹¤. `TFBertForSequenceClassification`ê³¼ ê°™ì€ ì‹ì´ë‹¤.

 ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ ì˜¬ ë•Œ  ëª¨ë¸ ì„¤ì •ì„ ìœ„í•´ `Config`ë¥¼ ì¸ìë¡œ ë„˜ê¸´ë‹¤. ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ëª©ì ì— ë”°ë¼ ë‹¤ë¥´ë¯€ë¡œ, ì´ ì—­ì‹œ documentationì„ ì°¸ê³ í•˜ë©´ ëœë‹¤.  BERT ëª¨ë¸ì€ `logits` (ë¡œì§“ ê°’)ì„ ë°˜í™˜í•œë‹¤.

```python
# ë¶„ë¥˜ ëª¨ë¸ config ì„¤ì •
my_config = BertConfig.from_pretrained(
    'bert-base-multilingual-cased',
    num_labels=2,
    output_hidden_states=False,
    output_attentions=False
)

# ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', config=my_config)
```



<br>

## 2. Kerasì—ì„œ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°



 ì—…ë°ì´íŠ¸ëœ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ê°€ì¥ ë§ˆìŒì— ë“œëŠ” ë¶€ë¶„ì´ë‹¤. `TF`ê°€ ë¶™ì–´ì„œ Tensorflow ë²„ì „ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì€ `tf.keras.Model` í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ëŠ”ë‹¤. Kerasì—ì„œ ëª¨ë¸ì„ ì„¤ì •í•˜ê³  ì»¤ìŠ¤í…€í•˜ëŠ” ë°©ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 

 ì–¸ì–´ ëª¨ë¸ì—ì„œ ì„ë² ë”© ë ˆì´ì–´ í™œìš©, ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ë“±ì— ë”°ë¼, í¬ê²Œ ì‚¬ìš©ë²•ì„ ë‹¤ìŒê³¼ ê°™ì´ **ì„¸ ê°€ì§€**ë¡œ ì •ë¦¬í•´ ë³´ì•˜ë‹¤. ë„¤ì´ë²„ ì˜í™” ê°ì„±ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì. (ì „ì²˜ë¦¬ëŠ” ì´ë¯¸ ì§„í–‰í–ˆë‹¤ê³  ê°€ì •í•œë‹¤.)

<br>

### ë°”ë¡œ ì‚¬ìš©í•˜ê¸°

 ê°€ì¥ ì‰¬ìš´ ì‚¬ìš©ë²•ì´ë‹¤. ì‚¬ì „í•™ìŠµëœ BERT ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ë°”ë¡œ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•œ ë’¤, Dense ì¸µë§Œ ì–¹ëŠ”ë‹¤.

```python
# ëª¨ë¸ ë„¤íŠ¸ì›Œí¬ ì„¤ì •
input_ids = Input(batch_shape=(None, MAX_SEQUENCE_LEN), dtype=tf.int32, name='input_ids')
input_masks = Input(batch_shape=(None, MAX_SEQUENCE_LEN), dtype=tf.int32, name='attention_masks')
embedding = bert_model([input_ids, input_masks])[0] # logitê°’ ë°˜í™˜
y_output = Dense(1, activation='sigmoid')(embedding) # sigmoid: ì´ì§„ ë¶„ë¥˜

# ëª¨ë¸ êµ¬ì„± ë° ì»´íŒŒì¼
model = Model(inputs=[input_ids, attention_masks], outputs=output, name='Bert_Classification_1')
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='binary_crossentropy',
              metrics=['acc'])
```



 ëª¨ë¸ ì „ì²´ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. 

![UseDirectly]({{site.url}}/assets/images/huggingface1.svg){: width="500"}{: .align-center}



```python
Model: "Bert_Classification"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 64)]         0                                            
__________________________________________________________________________________________________
attention_masks (InputLayer)    [(None, 64)]         0                                            
__________________________________________________________________________________________________
tf_bert_for_sequence_classifica ((None, 2),)         177854978   input_ids[0][0]                  
                                                                 attention_masks[0][0]            
__________________________________________________________________________________________________
dense (Dense)                   (None, 1)            3           tf_bert_for_sequence_classificati
==================================================================================================
Total params: 177,854,981
Trainable params: 177,854,981
Non-trainable params: 0
__________________________________________________________________________________________________
None
```

<br>

### Embedding ë ˆì´ì–´ ì¶”ì¶œ í›„ ì‚¬ìš©í•˜ê¸°

 ì‚¬ì „í•™ìŠµëœ BERT ëª¨ë¸ì—ì„œ latent featureë¡œì„œì˜ ì„ë² ë”© ë ˆì´ì–´ë§Œ ì¶”ì¶œí•˜ì—¬ ì‚¬ìš©í•œë‹¤. Kerasì—ì„œ Embedding ë ˆì´ì–´ë¥¼ ì‚¬ì „í•™ìŠµí•˜ì˜€ì„ ë•Œ, `trainable=False` ì˜µì…˜ì„ ì£¼ì—ˆë˜ ê²ƒì„ ìƒê¸°í•˜ë©´ ëœë‹¤. ì…ë ¥ ë° ì„ë² ë”©ì— ëŒ€í•œ train ì„¤ì •ì„ í•´ì œí•˜ë©´ ëœë‹¤. ì´í›„ ì¸µì€ ììœ ë¡­ê²Œ êµ¬ì„±í•œë‹¤.

```python
# ì…ë ¥ì¸µ
input_ids = Input(batch_shape=(None, MAX_SEQUENCE_LEN), dtype=tf.int32, name='input_ids')
input_masks = Input(batch_shape=(None, MAX_SEQUENCE_LEN), dtype=tf.int32, name='attention_masks')

# latent feature ì¶”ì¶œ
embedding = bert_model(input_ids, attention_mask=input_masks)[0]
cls_tokens = embedding[:, 0, :]

# train í•´ì œ
input_ids.trainable = False
input_masks.trainable = False
embedding.trainble = False

# ì¸µ ìŒ“ê¸°
X_latent = BatchNormalization()(cls_tokens)
X_dense = Dense(192, activation='relu')(X_latent)
X_dense = Dropout(0.3)(X_dense)
y_output = Dense(1, activation='sigmoid')(X_dense)

# ëª¨ë¸ êµ¬ì„± ë° ì»´íŒŒì¼
model = Model(inputs=[input_ids, input_masks], outputs=y_output, name='Bert_Classification_2')
model.compile(optimizer=Adam(learning_rate=3e-5),
              loss='binary_crossentropy',
              metrics=['acc'])
```



ëª¨ë¸ ì „ì²´ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. 

![latentExtract]({{site.url}}/assets/images/huggingface2.svg){: width="500"}{: .align-center}

```python
Model: "Bert_Classification_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 64)]         0                                            
__________________________________________________________________________________________________
attention_masks (InputLayer)    [(None, 64)]         0                                            
__________________________________________________________________________________________________
tf_bert_model (TFBertModel)     ((None, 64, 768), (N 177853440   input_ids[0][0]                  
                                                                 attention_masks[0][0]            
__________________________________________________________________________________________________
tf_op_layer_strided_slice (Tens [(None, 768)]        0           tf_bert_model[6][0]              
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 768)          3072        tf_op_layer_strided_slice[0][0]  
__________________________________________________________________________________________________
dense (Dense)                   (None, 192)          147648      batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 192)          0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            193         dropout[0][0]                    
==================================================================================================
Total params: 178,004,353
Trainable params: 178,002,817
Non-trainable params: 1,536
__________________________________________________________________________________________________
None
```





<br>

### Embedding ë ˆì´ì–´ ì¶”ì¶œ í›„ Fine-Tune

 Embedding ë ˆì´ì–´ë¥¼ ì¶”ì¶œí•œ í›„, í•´ë‹¹ Embedding ë ˆì´ì–´ì— ë˜ ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ë¥¼ ì ìš©í•˜ì—¬ *Fine-Tuning* ê³¼ì •ì„ ê±°ì¹  ìˆ˜ë„ ìˆë‹¤. 

```python
# ì…ë ¥ì¸µ
input_ids = Input(batch_shape=(None, MAX_SEQUENCE_LEN), dtype=tf.int32, name='input_ids')
input_masks = Input(batch_shape=(None, MAX_SEQUENCE_LEN), dtype=tf.int32, name='attention_masks')

# latent feature ì¶”ì¶œ
embedding = bert_model(input_ids, attention_mask=input_masks)[0]

# train í•´ì œ
input_ids.trainable = False
input_masks.trainable = False
embedding.trainble = False

# embedding layer fine-tune
X_embed = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding)
X_embed = GlobalMaxPool1D()(X_embed)  

# ì¸µ ìŒ“ê¸°
X_latent = BatchNormalization()(X_embed)
X_dense = Dense(192, activation='relu')(X_latent)
X_dense = Dropout(0.3)(X_dense)
y_output = Dense(1, activation='sigmoid')(X_dense)

# ëª¨ë¸ êµ¬ì„± ë° ì»´íŒŒì¼
model = Model(inputs=[input_ids, input_masks], outputs=y_output, name='Bert_Classification_2')
model.compile(optimizer=Adam(learning_rate=3e-5),
              loss='binary_crossentropy',
              metrics=['acc'])
```



ëª¨ë¸ ì „ì²´ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. 

![latentExtractFineTune]({{site.url}}/assets/images/huggingface3.svg){: width="500"}{: .align-center}

```python
Model: "Bert_Classification_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 64)]         0                                            
__________________________________________________________________________________________________
attention_masks (InputLayer)    [(None, 64)]         0                                            
__________________________________________________________________________________________________
tf_bert_model (TFBertModel)     ((None, 64, 768), (N 177853440   input_ids[0][0]                  
                                                                 attention_masks[0][0]            
__________________________________________________________________________________________________
bidirectional (Bidirectional)   (None, 64, 100)      327600      tf_bert_model[0][0]              
__________________________________________________________________________________________________
global_max_pooling1d (GlobalMax (None, 100)          0           bidirectional[0][0]              
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 100)          400         global_max_pooling1d[0][0]       
__________________________________________________________________________________________________
dense (Dense)                   (None, 192)          19392       batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 192)          0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            193         dropout[0][0]                    
==================================================================================================
Total params: 178,201,025
Trainable params: 178,200,825
Non-trainable params: 200
__________________________________________________________________________________________________
None
```

<br>

### ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡

 `model.fit`, `model.predict`ì™€ ê°™ì´ Keras ë°©ì‹ëŒ€ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•˜ë©´ ëœë‹¤. ë‹¤ë§Œ, ì…ë ¥ ì¸µì— ì¸ìë¡œ `input_id`ì™€ `mask`ë¥¼ í•¨ê»˜ ë„˜ê²¨ ì£¼ì–´ì•¼ í•œë‹¤. (* ì•„ì£¼ ë‹¹ì—°í•˜ê² ì§€ë§Œ, ë§Œì•½ `token_type_ids`ê°€ ìˆë‹¤ë©´ ì´ ì—­ì‹œ ë„˜ê²¨ ì£¼ë„ë¡ ëª¨ë¸ì„ êµ¬ì„±í•´ì•¼ í•œë‹¤.*)

```python
# í›ˆë ¨
hist = model.fit([X_train, X_train_masks], y_train,
                 validation_data=([X_test, X_test_masks], y_test),
                 batch_size=128,
                 epochs=3,
                 shuffle=True)

# ì˜ˆì¸¡
y_pred = model.predict([X_test, X_test_masks])
y_pred = np.where(y_pred > 0.5, 1, 0).reshape(-1, 1)
print('Accuracy = %.4f' % np.mean(y_test == y_pred))
```

